{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5819a4f-627e-46c7-b41e-e5544e9f32b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Context:\n",
    "    \"\"\"Context to save variables for backward computation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.saved_tensors = ()\n",
    "\n",
    "    def save_for_backward(self, *tensors):\n",
    "        self.saved_tensors = tuple(tensors)\n",
    "\n",
    "\n",
    "class Op:\n",
    "    @staticmethod\n",
    "    def forward(ctx, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @classmethod\n",
    "    def apply(cls, *inputs):\n",
    "        ctx = Context()\n",
    "        raw_inputs = [x.data if isinstance(x, Tensor) else x for x in inputs]\n",
    "        result_data = cls.forward(ctx, *raw_inputs)\n",
    "\n",
    "        requires_grad = any(isinstance(x, Tensor) and x.requires_grad for x in inputs)\n",
    "        out = Tensor(result_data, requires_grad=requires_grad)\n",
    "        if requires_grad:\n",
    "            out.grad_fn = (cls, ctx, inputs)\n",
    "        return out\n",
    "\n",
    "\n",
    "def reduce_grad_to_shape(grad, shape):\n",
    "    \"\"\"\n",
    "    Match the gradient dimension for the input tensor\n",
    "    - grad: output gradient\n",
    "    - shape: original input tensor shape\n",
    "    \"\"\"\n",
    "    # remove expanded dimension for batched data\n",
    "    while grad.ndim > len(shape):\n",
    "        grad = grad.sum(axis=0)\n",
    "\n",
    "    # remove the broadcasted axis\n",
    "    for i, dim in enumerate(shape):\n",
    "        if dim == 1 and (grad.shape[i] != 1):\n",
    "            grad = grad.sum(axis=i, keepdims=True)\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "class Add(Op):\n",
    "    @staticmethod\n",
    "    def forward(ctx, a, b):\n",
    "        ctx.save_for_backward(a.shape, b.shape)\n",
    "        return a + b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a_shape, b_shape = ctx.saved_tensors\n",
    "        grad_a = reduce_grad_to_shape(grad_output, a_shape)\n",
    "        grad_b = reduce_grad_to_shape(grad_output, b_shape)\n",
    "        return grad_a, grad_b\n",
    "\n",
    "\n",
    "class Mul(Op):\n",
    "    @staticmethod\n",
    "    def forward(ctx, a, b):\n",
    "        ctx.save_for_backward(a, b)\n",
    "        return a * b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_a = reduce_grad_to_shape(b * grad_output, a.shape)\n",
    "        grad_b = reduce_grad_to_shape(a * grad_output, b.shape)\n",
    "        return grad_a, grad_b\n",
    "\n",
    "\n",
    "class MatMul(Op):\n",
    "    @staticmethod\n",
    "    def forward(ctx, a, b):\n",
    "        ctx.save_for_backward(a, b)\n",
    "        return a @ b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_a = grad_output @ b.T\n",
    "        grad_b = a.T @ grad_output\n",
    "        return grad_a, grad_b\n",
    "\n",
    "\n",
    "class Pow(Op):\n",
    "    @staticmethod\n",
    "    def forward(ctx, a, b):\n",
    "        ctx.save_for_backward(a, b)\n",
    "        return a**b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_a = grad_output * a ** (b - 1) * b\n",
    "        grad_b = grad_output * a**b * np.log(a)\n",
    "        return grad_a, grad_b\n",
    "\n",
    "\n",
    "class Sum(Op):\n",
    "    @staticmethod\n",
    "    def forward(ctx, a, axis=None, keepdims=False):\n",
    "        ctx.save_for_backward(a.shape, axis, keepdims)\n",
    "        return np.sum(a, axis=axis, keepdims=keepdims)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        (a_shape, axis, keepdims) = ctx.saved_tensors\n",
    "\n",
    "        # Restore reduced dimensions\n",
    "        if not keepdims and axis is not None:\n",
    "            to_expand = axis\n",
    "            grad_output = np.expand_dims(grad_output, to_expand)\n",
    "\n",
    "        grad_a = np.broadcast_to(grad_output, a_shape)\n",
    "        return grad_a\n",
    "\n",
    "\n",
    "class ReLU(Op):\n",
    "    @staticmethod\n",
    "    def forward(ctx, a):\n",
    "        ctx.save_for_backward(a)\n",
    "        return np.maximum(a, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        (a,) = ctx.saved_tensors\n",
    "        grad_a = (a > 0) * grad_output\n",
    "        return reduce_grad_to_shape(grad_a, a.shape)\n",
    "\n",
    "\n",
    "class CrossEntropyLoss(Op):\n",
    "    @staticmethod\n",
    "    def forward(ctx, logits, targets):\n",
    "        assert logits.ndim <= 2, \"Multidimensional classes not supported\"\n",
    "\n",
    "        # Log-Softmax, see https://arxiv.org/pdf/1909.03469\n",
    "        logits_max = logits.max(axis=-1, keepdims=True)\n",
    "        logits_shifted = logits - logits_max\n",
    "        logsumexp = np.log(np.sum(np.exp(logits_shifted), axis=-1, keepdims=True))\n",
    "        log_probs = logits_shifted - logsumexp\n",
    "\n",
    "        losses = -np.sum(targets * log_probs, axis=-1)\n",
    "        ctx.save_for_backward(log_probs, targets, np.size(losses))\n",
    "\n",
    "        return np.mean(losses)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        log_probs, targets, batch_size = ctx.saved_tensors\n",
    "\n",
    "        grad_logits = (\n",
    "            grad_output\n",
    "            * (targets.sum(-1, keepdims=True) * np.exp(log_probs) - targets)\n",
    "            / batch_size\n",
    "        )\n",
    "        grad_targets = grad_output * -log_probs / batch_size\n",
    "        return grad_logits, grad_targets\n",
    "\n",
    "\n",
    "class Log(Op):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return np.log(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        (x,) = ctx.saved_tensors\n",
    "        grad_x = grad_output * (1.0 / x)\n",
    "        return reduce_grad_to_shape(grad_x, x.shape)\n",
    "\n",
    "\n",
    "class NLLLoss(Op):\n",
    "    @staticmethod\n",
    "    def forward(ctx, log_probs, targets_one_hot):\n",
    "        assert log_probs.ndim <= 2, \"Multidimensional classes not supported\"\n",
    "\n",
    "        # PyTorch batches on dim 0 or 1\n",
    "        losses = -np.sum(targets_one_hot * log_probs, axis=-1)\n",
    "        ctx.save_for_backward(log_probs, targets_one_hot, np.size(losses))\n",
    "\n",
    "        return np.mean(losses)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        log_probs, targets_one_hot, batch_size = ctx.saved_tensors\n",
    "\n",
    "        grad_log_probs = grad_output * -targets_one_hot / batch_size\n",
    "        grad_targets = grad_output * -log_probs / batch_size\n",
    "\n",
    "        return grad_log_probs, grad_targets\n",
    "\n",
    "\n",
    "class Softmax(Op):\n",
    "    @staticmethod\n",
    "    def forward(ctx, logits):\n",
    "        # Shifting for stability, see https://arxiv.org/pdf/1909.03469\n",
    "        logits_max = logits.max(axis=-1, keepdims=True)\n",
    "        logits_exp_shifted = np.exp(logits - logits_max)\n",
    "        probs = logits_exp_shifted / logits_exp_shifted.sum(axis=-1, keepdims=True)\n",
    "        ctx.save_for_backward(probs)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        (probs,) = ctx.saved_tensors\n",
    "        # https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "        # https://github.com/jax-ml/jax/blob/f74467851b1186b434d4b538d0be419378a47a69/jax/_src/nn/functions.py#L648-L652\n",
    "        return probs * (grad_output - (probs * grad_output).sum(axis=-1, keepdims=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "625bfb31-eebe-4060-9a05-fa2df6bfa2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, requires_grad=False):\n",
    "        if isinstance(data, Tensor):\n",
    "            data = data.data  # unwrap\n",
    "        self.data = np.array(data, dtype=np.float32)\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = None\n",
    "        self.grad_fn = None  # (OpClass, ctx, inputs) for non-leaf tensors\n",
    "\n",
    "    # Operator overloads delegate to our Op classes via apply:\n",
    "    def __add__(self, other):\n",
    "        return Add.apply(self, other)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return Add.apply(other, self)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return Add.apply(self, Mul.apply(other, Tensor(-1.0)))\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return Add.apply(other, Mul.apply(self, Tensor(-1.0)))\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Mul.apply(self, other)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return Mul.apply(other, self)\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        return MatMul.apply(self, other)  # matrix @\n",
    "\n",
    "    def __neg__(self):\n",
    "        return Mul.apply(self, Tensor(-1.0))\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return Mul.apply(self, Tensor(1.0) / other)\n",
    "\n",
    "    def __pow__(self, exponent):\n",
    "        return Pow.apply(self, exponent)\n",
    "\n",
    "    def __rpow__(self, base):\n",
    "        return Pow.apply(base, self)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(data={self.data}, grad={self.grad}, requires_grad={self.requires_grad})\"\n",
    "\n",
    "    # initialization\n",
    "    @staticmethod\n",
    "    def zeros(shape, requires_grad=False):\n",
    "        return Tensor(np.zeros(shape, dtype=np.float32), requires_grad=requires_grad)\n",
    "\n",
    "    @staticmethod\n",
    "    def ones(shape, requires_grad=False):\n",
    "        return Tensor(np.ones(shape, dtype=np.float32), requires_grad=requires_grad)\n",
    "\n",
    "    def backward(self, grad_output=None):\n",
    "        if not self.requires_grad:\n",
    "            raise RuntimeError(\n",
    "                \"Cannot call backward on a tensor that does not require grad.\"\n",
    "            )\n",
    "        # If no grad specified, tensor must be scalar\n",
    "        if grad_output is None:\n",
    "            if self.data.size != 1:\n",
    "                raise RuntimeError(\n",
    "                    \"grad_output must be provided for non-scalar tensors\"\n",
    "                )\n",
    "            grad_output = np.ones_like(self.data, dtype=np.float32)\n",
    "        else:\n",
    "            grad_output = np.array(grad_output, dtype=np.float32)\n",
    "        # Initialize this tensor's gradient\n",
    "        self.grad = grad_output\n",
    "\n",
    "        # Build a topologically sorted list of tensors (post-order DFS)\n",
    "        topo_order = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_graph(t):\n",
    "            if isinstance(t, Tensor) and t not in visited:\n",
    "                visited.add(t)\n",
    "                if t.grad_fn is not None:  # not a leaf\n",
    "                    op_cls, ctx, inputs = t.grad_fn\n",
    "                    for inp in inputs:\n",
    "                        build_graph(inp)\n",
    "                topo_order.append(t)\n",
    "\n",
    "        build_graph(self)\n",
    "\n",
    "        # Traverse graph in reverse topological order, apply chain rule\n",
    "        for t in reversed(topo_order):\n",
    "            if t.grad_fn is None:\n",
    "                continue  # leaf node (no backward op)\n",
    "            op_cls, ctx, inputs = t.grad_fn\n",
    "            grad_out = t.grad  # gradient of the output w.rt. this tensor\n",
    "            # Compute gradients of inputs via this op's backward\n",
    "            grad_inputs = op_cls.backward(ctx, grad_out)\n",
    "            if grad_inputs is None:\n",
    "                grad_inputs = ()\n",
    "            elif not isinstance(grad_inputs, tuple):\n",
    "                grad_inputs = (grad_inputs,)\n",
    "            # Accumulate gradients into input tensors\n",
    "            for inp, grad in zip(inputs, grad_inputs):\n",
    "                if isinstance(inp, Tensor) and inp.requires_grad and grad is not None:\n",
    "                    grad = np.array(grad, dtype=np.float32)  # ensure numpy\n",
    "                    if inp.grad is None:\n",
    "                        inp.grad = grad\n",
    "                    else:\n",
    "                        inp.grad += grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b23dd492-74d7-4765-af1c-b64f031e0775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Autograd Graph (parents -> node) ===\n",
      "[34650] Leaf(shape=(), requires_grad=True)\n",
      "[4dc40] Leaf(shape=(), requires_grad=True)\n",
      "[34650] Leaf(shape=(), requires_grad=True) --Mul--> [b9dc0] Mul(out_shape=())\n",
      "[4dc40] Leaf(shape=(), requires_grad=True) --Mul--> [b9dc0] Mul(out_shape=())\n",
      "[8c440] Leaf(shape=(), requires_grad=True)\n",
      "[b9dc0] Mul(out_shape=()) --Add--> [c8d40] Add(out_shape=())\n",
      "[8c440] Leaf(shape=(), requires_grad=True) --Add--> [c8d40] Add(out_shape=())\n",
      "[c8d40] Add(out_shape=()) --ReLU--> [37da0] ReLU(out_shape=())\n",
      "\n",
      "=== Backward visit order (reverse topological) ===\n",
      "[37da0] ReLU(out_shape=())\n",
      "[c8d40] Add(out_shape=())\n",
      "[8c440] Leaf(shape=(), requires_grad=True)\n",
      "[b9dc0] Mul(out_shape=())\n",
      "[4dc40] Leaf(shape=(), requires_grad=True)\n",
      "[34650] Leaf(shape=(), requires_grad=True)\n",
      "\n",
      "=== Gradients ===\n",
      "dl/dx = 3.0\n",
      "dl/dw = 2.0\n",
      "dl/db = 1.0\n"
     ]
    }
   ],
   "source": [
    "def _node_name(t: \"Tensor\"):\n",
    "    if t.grad_fn is None:\n",
    "        return f\"Leaf(shape={tuple(t.data.shape)}, requires_grad={t.requires_grad})\"\n",
    "    op_cls, _, _ = t.grad_fn\n",
    "    return f\"{op_cls.__name__}(out_shape={tuple(t.data.shape)})\"\n",
    "\n",
    "\n",
    "def _short_id(obj):\n",
    "    return hex(id(obj))[-5:]  # 보기 좋은 짧은 id\n",
    "\n",
    "\n",
    "def trace_graph(root: \"Tensor\"):\n",
    "    \"\"\"루트 텐서에서 시작해 모든 조상(부모 텐서)들을 수집하고\n",
    "    토폴로지 순서를 반환합니다.\"\"\"\n",
    "    visited = set()\n",
    "    topo = []\n",
    "\n",
    "    def build(t):\n",
    "        if not isinstance(t, Tensor) or t in visited:\n",
    "            return\n",
    "        visited.add(t)\n",
    "        if t.grad_fn is not None:\n",
    "            op_cls, ctx, inputs = t.grad_fn\n",
    "            for inp in inputs:\n",
    "                if isinstance(inp, Tensor):\n",
    "                    build(inp)\n",
    "        topo.append(t)\n",
    "\n",
    "    build(root)\n",
    "    return topo  # 부모가 먼저, root가 마지막(포스트오더)\n",
    "\n",
    "\n",
    "def print_autograd_graph(root: \"Tensor\"):\n",
    "    topo = trace_graph(root)\n",
    "    print(\"=== Autograd Graph (parents -> node) ===\")\n",
    "    for node in topo:\n",
    "        label_node = _node_name(node)\n",
    "        nid = _short_id(node)\n",
    "        if node.grad_fn is None:\n",
    "            print(f\"[{nid}] {label_node}\")\n",
    "            continue\n",
    "        op_cls, _, inputs = node.grad_fn\n",
    "        # 부모들 나열 (부모) --op--> (자식=node)\n",
    "        for inp in inputs:\n",
    "            if isinstance(inp, Tensor):\n",
    "                pid = _short_id(inp)\n",
    "                print(\n",
    "                    f\"[{pid}] {_node_name(inp)} --{op_cls.__name__}--> [{nid}] {label_node}\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"[const {inp}] --{op_cls.__name__}--> [{nid}] {label_node}\")\n",
    "\n",
    "    # backward 방문 순서(실제로 backward에서 사용하는 역토폴로지)\n",
    "    print(\"\\n=== Backward visit order (reverse topological) ===\")\n",
    "    for node in reversed(topo):\n",
    "        print(f\"[{_short_id(node)}] {_node_name(node)}\")\n",
    "\n",
    "\n",
    "# f(x, w, b) = ReLU(x*w + b)\n",
    "x = Tensor(2.0, requires_grad=True)\n",
    "w = Tensor(3.0, requires_grad=True)\n",
    "b = Tensor(-4.0, requires_grad=True)\n",
    "\n",
    "z = x * w + b  # Add(Mul(x,w), b)\n",
    "y = ReLU.apply(z)  # ReLU(z)\n",
    "loss = y  # 스칼라처럼 취급 (여기선 y가 스칼라)\n",
    "\n",
    "print_autograd_graph(loss)\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\n=== Gradients ===\")\n",
    "print(\"dl/dx =\", x.grad)  # ReLU'(z) * w  (z>0이면 w, 아니면 0)\n",
    "print(\"dl/dw =\", w.grad)  # ReLU'(z) * x\n",
    "print(\"dl/db =\", b.grad)  # ReLU'(z) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13d469f0-5a90-4ffa-99c7-127955807281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Autograd Graph (parents -> node) ===\n",
      "[43d70] Leaf(shape=(), requires_grad=True)\n",
      "[8dc40] Leaf(shape=(), requires_grad=True)\n",
      "[43d70] Leaf(shape=(), requires_grad=True) --Mul--> [41e20] Mul(out_shape=())\n",
      "[8dc40] Leaf(shape=(), requires_grad=True) --Mul--> [41e20] Mul(out_shape=())\n",
      "[40ef0] Leaf(shape=(), requires_grad=True)\n",
      "[41e20] Mul(out_shape=()) --Add--> [40830] Add(out_shape=())\n",
      "[40ef0] Leaf(shape=(), requires_grad=True) --Add--> [40830] Add(out_shape=())\n",
      "[40830] Add(out_shape=()) --ReLU--> [402c0] ReLU(out_shape=())\n",
      "\n",
      "=== Backward visit order (reverse topological) ===\n",
      "[402c0] ReLU(out_shape=())\n",
      "[40830] Add(out_shape=())\n",
      "[40ef0] Leaf(shape=(), requires_grad=True)\n",
      "[41e20] Mul(out_shape=())\n",
      "[8dc40] Leaf(shape=(), requires_grad=True)\n",
      "[43d70] Leaf(shape=(), requires_grad=True)\n",
      "\n",
      "=== Gradients ===\n",
      "dl/dx = 3.0\n",
      "dl/dw = 2.0\n",
      "dl/db = 1.0\n"
     ]
    }
   ],
   "source": [
    "x = Tensor(2.0, requires_grad=True)\n",
    "w = Tensor(3.0, requires_grad=True)\n",
    "b = Tensor(-4.0, requires_grad=True)\n",
    "\n",
    "z = x * w + b  # Add(Mul(x,w), b)\n",
    "y = ReLU.apply(z)  # ReLU(z)\n",
    "loss = y  # 스칼라처럼 취급 (여기선 y가 스칼라)\n",
    "\n",
    "print_autograd_graph(loss)\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\n=== Gradients ===\")\n",
    "print(\"dl/dx =\", x.grad)  # ReLU'(z) * w  (z>0이면 w, 아니면 0)\n",
    "print(\"dl/dw =\", w.grad)  # ReLU'(z) * x\n",
    "print(\"dl/db =\", b.grad)  # ReLU'(z) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "601e4d2b-ca5f-4811-82ba-a37e00555854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Autograd Graph (parents -> node) ===\n",
      "[42930] Leaf(shape=(3,), requires_grad=True)\n",
      "[42450] Leaf(shape=(), requires_grad=False)\n",
      "[42930] Leaf(shape=(3,), requires_grad=True) --Mul--> [43650] Mul(out_shape=(3,))\n",
      "[42450] Leaf(shape=(), requires_grad=False) --Mul--> [43650] Mul(out_shape=(3,))\n",
      "[42a50] Leaf(shape=(), requires_grad=False)\n",
      "[43650] Mul(out_shape=(3,)) --Add--> [43170] Add(out_shape=(3,))\n",
      "[42a50] Leaf(shape=(), requires_grad=False) --Add--> [43170] Add(out_shape=(3,))\n",
      "[43170] Add(out_shape=(3,)) --ReLU--> [42f90] ReLU(out_shape=(3,))\n",
      "[42f90] ReLU(out_shape=(3,)) --Sum--> [43a70] Sum(out_shape=())\n",
      "\n",
      "=== Backward visit order (reverse topological) ===\n",
      "[43a70] Sum(out_shape=())\n",
      "[42f90] ReLU(out_shape=(3,))\n",
      "[43170] Add(out_shape=(3,))\n",
      "[42a50] Leaf(shape=(), requires_grad=False)\n",
      "[43650] Mul(out_shape=(3,))\n",
      "[42450] Leaf(shape=(), requires_grad=False)\n",
      "[42930] Leaf(shape=(3,), requires_grad=True)\n",
      "\n",
      "=== Values & Gradients ===\n",
      "y = [0. 1. 0.]\n",
      "dl/da = [0. 2. 0.]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(np.array([0.4, 1.0, -2.0], dtype=np.float32), requires_grad=True)\n",
    "\n",
    "y = ReLU.apply(a * Tensor(2.0) - Tensor(1.0))  # elementwise\n",
    "loss = Sum.apply(y)  # Sum op로 스칼라화\n",
    "\n",
    "print_autograd_graph(loss)\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\n=== Values & Gradients ===\")\n",
    "print(\"y =\", y.data)  # [0., 1., 0.]  (ReLU 결과)\n",
    "print(\"dl/da =\", a.grad)  # ReLU>0인 위치에서 2, 나머지는 0  → [0., 2., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7eecd87-5df5-40dd-93f1-f909d28871bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST data loaded: (60000, 784) (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import os, gzip\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "\n",
    "def load_mnist(path=None):\n",
    "    \"\"\"Download MNIST and load it into NumPy arrays.\"\"\"\n",
    "    # url_base = \"http://yann.lecun.com/exdb/mnist/\"\n",
    "    url_base = \"https://ossci-datasets.s3.amazonaws.com/mnist/\"\n",
    "\n",
    "    files = {\n",
    "        \"train_images\": \"train-images-idx3-ubyte.gz\",\n",
    "        \"train_labels\": \"train-labels-idx1-ubyte.gz\",\n",
    "        \"test_images\": \"t10k-images-idx3-ubyte.gz\",\n",
    "        \"test_labels\": \"t10k-labels-idx1-ubyte.gz\",\n",
    "    }\n",
    "    # Default path to ./data/mnist\n",
    "    if path is None:\n",
    "        path = os.path.join(os.path.expanduser(\"./\"), \"data\", \"mnist\")\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Download missing files\n",
    "    for name, filename in files.items():\n",
    "        filepath = os.path.join(path, filename)\n",
    "        if not os.path.isfile(filepath):\n",
    "            urlretrieve(url_base + filename, filepath)\n",
    "            print(f\"Downloaded {filename}\")\n",
    "\n",
    "    # Load images\n",
    "    def _read_images(filename):\n",
    "        with gzip.open(os.path.join(path, filename), \"rb\") as f:\n",
    "            data = f.read()\n",
    "            # The first 16 bytes are header (magic, num, rows, cols)\n",
    "            images = np.frombuffer(data, dtype=np.uint8, offset=16)\n",
    "        images = images.reshape(-1, 28 * 28).astype(np.float32) / 255.0\n",
    "        return images\n",
    "\n",
    "    # Load labels\n",
    "    def _read_labels(filename):\n",
    "        with gzip.open(os.path.join(path, filename), \"rb\") as f:\n",
    "            data = f.read()\n",
    "            # First 8 bytes are header (magic, num)\n",
    "            labels = np.frombuffer(data, dtype=np.uint8, offset=8)\n",
    "        # Convert to one-hot vectors of length 10\n",
    "        one_hot = np.zeros((labels.size, 10), dtype=np.float32)\n",
    "        one_hot[np.arange(labels.size), labels] = 1.0\n",
    "        return one_hot\n",
    "\n",
    "    # Read all parts\n",
    "    X_train = _read_images(files[\"train_images\"])\n",
    "    y_train = _read_labels(files[\"train_labels\"])\n",
    "    X_test = _read_images(files[\"test_images\"])\n",
    "    y_test = _read_labels(files[\"test_labels\"])\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "# Load the MNIST data (this will download if not already present)\n",
    "X_train, y_train, X_test, y_test = load_mnist()\n",
    "print(\"MNIST data loaded:\", X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dab29554-1619-432e-87e5-7919b4e1de98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Itr 0 / 60000: avg loss = 0.003838\n",
      "Epoch 1, Itr 1000 / 60000: avg loss = 0.000042\n",
      "Epoch 1, Itr 2000 / 60000: avg loss = 0.000040\n",
      "Epoch 1, Itr 3000 / 60000: avg loss = 0.000040\n",
      "Epoch 1, Itr 4000 / 60000: avg loss = 0.000039\n",
      "Epoch 1, Itr 5000 / 60000: avg loss = 0.000039\n",
      "Epoch 1, Itr 6000 / 60000: avg loss = 0.000039\n",
      "Epoch 1, Itr 7000 / 60000: avg loss = 0.000039\n",
      "Epoch 1, Itr 8000 / 60000: avg loss = 0.000039\n",
      "Epoch 1, Itr 9000 / 60000: avg loss = 0.000039\n",
      "Epoch 1, Itr 10000 / 60000: avg loss = 0.000039\n",
      "Epoch 1, Itr 11000 / 60000: avg loss = 0.000039\n",
      "Epoch 1, Itr 12000 / 60000: avg loss = 0.000039\n",
      "Epoch 1, Itr 13000 / 60000: avg loss = 0.000039\n",
      "Epoch 1, Itr 14000 / 60000: avg loss = 0.000039\n",
      "Epoch 1, Itr 15000 / 60000: avg loss = 0.000039\n",
      "Epoch 1, Itr 16000 / 60000: avg loss = 0.000039\n",
      "Epoch 1, Itr 17000 / 60000: avg loss = 0.000039\n",
      "Epoch 1, Itr 18000 / 60000: avg loss = 0.000039\n",
      "Epoch 1, Itr 19000 / 60000: avg loss = 0.000039\n",
      "Epoch 1, Itr 20000 / 60000: avg loss = 0.000039\n",
      "Epoch 1, Itr 21000 / 60000: avg loss = 0.000039\n",
      "Epoch 1, Itr 22000 / 60000: avg loss = 0.000038\n",
      "Epoch 1, Itr 23000 / 60000: avg loss = 0.000038\n",
      "Epoch 1, Itr 24000 / 60000: avg loss = 0.000038\n",
      "Epoch 1, Itr 25000 / 60000: avg loss = 0.000038\n",
      "Epoch 1, Itr 26000 / 60000: avg loss = 0.000038\n",
      "Epoch 1, Itr 27000 / 60000: avg loss = 0.000038\n",
      "Epoch 1, Itr 28000 / 60000: avg loss = 0.000038\n",
      "Epoch 1, Itr 29000 / 60000: avg loss = 0.000038\n",
      "Epoch 1, Itr 30000 / 60000: avg loss = 0.000038\n",
      "Epoch 1, Itr 31000 / 60000: avg loss = 0.000038\n",
      "Epoch 1, Itr 32000 / 60000: avg loss = 0.000038\n",
      "Epoch 1, Itr 33000 / 60000: avg loss = 0.000037\n",
      "Epoch 1, Itr 34000 / 60000: avg loss = 0.000037\n",
      "Epoch 1, Itr 35000 / 60000: avg loss = 0.000037\n",
      "Epoch 1, Itr 36000 / 60000: avg loss = 0.000037\n",
      "Epoch 1, Itr 37000 / 60000: avg loss = 0.000036\n",
      "Epoch 1, Itr 38000 / 60000: avg loss = 0.000036\n",
      "Epoch 1, Itr 39000 / 60000: avg loss = 0.000036\n",
      "Epoch 1, Itr 40000 / 60000: avg loss = 0.000035\n",
      "Epoch 1, Itr 41000 / 60000: avg loss = 0.000035\n",
      "Epoch 1, Itr 42000 / 60000: avg loss = 0.000034\n",
      "Epoch 1, Itr 43000 / 60000: avg loss = 0.000034\n",
      "Epoch 1, Itr 44000 / 60000: avg loss = 0.000034\n",
      "Epoch 1, Itr 45000 / 60000: avg loss = 0.000033\n",
      "Epoch 1, Itr 46000 / 60000: avg loss = 0.000033\n",
      "Epoch 1, Itr 47000 / 60000: avg loss = 0.000032\n",
      "Epoch 1, Itr 48000 / 60000: avg loss = 0.000032\n",
      "Epoch 1, Itr 49000 / 60000: avg loss = 0.000032\n",
      "Epoch 1, Itr 50000 / 60000: avg loss = 0.000031\n",
      "Epoch 1, Itr 51000 / 60000: avg loss = 0.000031\n",
      "Epoch 1, Itr 52000 / 60000: avg loss = 0.000030\n",
      "Epoch 1, Itr 53000 / 60000: avg loss = 0.000030\n",
      "Epoch 1, Itr 54000 / 60000: avg loss = 0.000030\n",
      "Epoch 1, Itr 55000 / 60000: avg loss = 0.000029\n",
      "Epoch 1, Itr 56000 / 60000: avg loss = 0.000029\n",
      "Epoch 1, Itr 57000 / 60000: avg loss = 0.000029\n",
      "Epoch 1, Itr 58000 / 60000: avg loss = 0.000028\n",
      "Epoch 1, Itr 59000 / 60000: avg loss = 0.000028\n",
      "Epoch 1 / Test accuracy: 80.38%\n",
      "Epoch 2, Itr 0 / 60000: avg loss = 0.000951\n",
      "Epoch 2, Itr 1000 / 60000: avg loss = 0.000012\n",
      "Epoch 2, Itr 2000 / 60000: avg loss = 0.000011\n",
      "Epoch 2, Itr 3000 / 60000: avg loss = 0.000011\n",
      "Epoch 2, Itr 4000 / 60000: avg loss = 0.000011\n",
      "Epoch 2, Itr 5000 / 60000: avg loss = 0.000011\n",
      "Epoch 2, Itr 6000 / 60000: avg loss = 0.000010\n",
      "Epoch 2, Itr 7000 / 60000: avg loss = 0.000010\n",
      "Epoch 2, Itr 8000 / 60000: avg loss = 0.000010\n",
      "Epoch 2, Itr 9000 / 60000: avg loss = 0.000010\n",
      "Epoch 2, Itr 10000 / 60000: avg loss = 0.000010\n",
      "Epoch 2, Itr 11000 / 60000: avg loss = 0.000010\n",
      "Epoch 2, Itr 12000 / 60000: avg loss = 0.000010\n",
      "Epoch 2, Itr 13000 / 60000: avg loss = 0.000010\n",
      "Epoch 2, Itr 14000 / 60000: avg loss = 0.000010\n",
      "Epoch 2, Itr 15000 / 60000: avg loss = 0.000010\n",
      "Epoch 2, Itr 16000 / 60000: avg loss = 0.000010\n",
      "Epoch 2, Itr 17000 / 60000: avg loss = 0.000010\n",
      "Epoch 2, Itr 18000 / 60000: avg loss = 0.000010\n",
      "Epoch 2, Itr 19000 / 60000: avg loss = 0.000010\n",
      "Epoch 2, Itr 20000 / 60000: avg loss = 0.000010\n",
      "Epoch 2, Itr 21000 / 60000: avg loss = 0.000010\n",
      "Epoch 2, Itr 22000 / 60000: avg loss = 0.000010\n",
      "Epoch 2, Itr 23000 / 60000: avg loss = 0.000009\n",
      "Epoch 2, Itr 24000 / 60000: avg loss = 0.000009\n",
      "Epoch 2, Itr 25000 / 60000: avg loss = 0.000009\n",
      "Epoch 2, Itr 26000 / 60000: avg loss = 0.000009\n",
      "Epoch 2, Itr 27000 / 60000: avg loss = 0.000009\n",
      "Epoch 2, Itr 28000 / 60000: avg loss = 0.000009\n",
      "Epoch 2, Itr 29000 / 60000: avg loss = 0.000009\n",
      "Epoch 2, Itr 30000 / 60000: avg loss = 0.000009\n",
      "Epoch 2, Itr 31000 / 60000: avg loss = 0.000009\n",
      "Epoch 2, Itr 32000 / 60000: avg loss = 0.000009\n",
      "Epoch 2, Itr 33000 / 60000: avg loss = 0.000009\n",
      "Epoch 2, Itr 34000 / 60000: avg loss = 0.000009\n",
      "Epoch 2, Itr 35000 / 60000: avg loss = 0.000009\n",
      "Epoch 2, Itr 36000 / 60000: avg loss = 0.000009\n",
      "Epoch 2, Itr 37000 / 60000: avg loss = 0.000009\n",
      "Epoch 2, Itr 38000 / 60000: avg loss = 0.000009\n",
      "Epoch 2, Itr 39000 / 60000: avg loss = 0.000009\n",
      "Epoch 2, Itr 40000 / 60000: avg loss = 0.000008\n",
      "Epoch 2, Itr 41000 / 60000: avg loss = 0.000008\n",
      "Epoch 2, Itr 42000 / 60000: avg loss = 0.000008\n",
      "Epoch 2, Itr 43000 / 60000: avg loss = 0.000008\n",
      "Epoch 2, Itr 44000 / 60000: avg loss = 0.000008\n",
      "Epoch 2, Itr 45000 / 60000: avg loss = 0.000008\n",
      "Epoch 2, Itr 46000 / 60000: avg loss = 0.000008\n",
      "Epoch 2, Itr 47000 / 60000: avg loss = 0.000008\n",
      "Epoch 2, Itr 48000 / 60000: avg loss = 0.000008\n",
      "Epoch 2, Itr 49000 / 60000: avg loss = 0.000008\n",
      "Epoch 2, Itr 50000 / 60000: avg loss = 0.000008\n",
      "Epoch 2, Itr 51000 / 60000: avg loss = 0.000008\n",
      "Epoch 2, Itr 52000 / 60000: avg loss = 0.000008\n",
      "Epoch 2, Itr 53000 / 60000: avg loss = 0.000008\n",
      "Epoch 2, Itr 54000 / 60000: avg loss = 0.000008\n",
      "Epoch 2, Itr 55000 / 60000: avg loss = 0.000008\n",
      "Epoch 2, Itr 56000 / 60000: avg loss = 0.000008\n",
      "Epoch 2, Itr 57000 / 60000: avg loss = 0.000008\n",
      "Epoch 2, Itr 58000 / 60000: avg loss = 0.000008\n",
      "Epoch 2, Itr 59000 / 60000: avg loss = 0.000008\n",
      "Epoch 2 / Test accuracy: 91.09%\n",
      "Epoch 3, Itr 0 / 60000: avg loss = 0.000824\n",
      "Epoch 3, Itr 1000 / 60000: avg loss = 0.000006\n",
      "Epoch 3, Itr 2000 / 60000: avg loss = 0.000006\n",
      "Epoch 3, Itr 3000 / 60000: avg loss = 0.000006\n",
      "Epoch 3, Itr 4000 / 60000: avg loss = 0.000006\n",
      "Epoch 3, Itr 5000 / 60000: avg loss = 0.000006\n",
      "Epoch 3, Itr 6000 / 60000: avg loss = 0.000006\n",
      "Epoch 3, Itr 7000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 8000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 9000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 10000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 11000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 12000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 13000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 14000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 15000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 16000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 17000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 18000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 19000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 20000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 21000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 22000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 23000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 24000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 25000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 26000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 27000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 28000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 29000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 30000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 31000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 32000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 33000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 34000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 35000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 36000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 37000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 38000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 39000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 40000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 41000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 42000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 43000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 44000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 45000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 46000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 47000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 48000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 49000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 50000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 51000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 52000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 53000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 54000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 55000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 56000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 57000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 58000 / 60000: avg loss = 0.000005\n",
      "Epoch 3, Itr 59000 / 60000: avg loss = 0.000005\n",
      "Epoch 3 / Test accuracy: 93.67%\n",
      "Epoch 4, Itr 0 / 60000: avg loss = 0.000264\n",
      "Epoch 4, Itr 1000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 2000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 3000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 4000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 5000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 6000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 7000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 8000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 9000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 10000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 11000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 12000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 13000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 14000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 15000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 16000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 17000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 18000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 19000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 20000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 21000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 22000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 23000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 24000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 25000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 26000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 27000 / 60000: avg loss = 0.000004\n",
      "Epoch 4, Itr 28000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 29000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 30000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 31000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 32000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 33000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 34000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 35000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 36000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 37000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 38000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 39000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 40000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 41000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 42000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 43000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 44000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 45000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 46000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 47000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 48000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 49000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 50000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 51000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 52000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 53000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 54000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 55000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 56000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 57000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 58000 / 60000: avg loss = 0.000003\n",
      "Epoch 4, Itr 59000 / 60000: avg loss = 0.000003\n",
      "Epoch 4 / Test accuracy: 95.23%\n",
      "Epoch 5, Itr 0 / 60000: avg loss = 0.000310\n",
      "Epoch 5, Itr 1000 / 60000: avg loss = 0.000004\n",
      "Epoch 5, Itr 2000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 3000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 4000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 5000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 6000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 7000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 8000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 9000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 10000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 11000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 12000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 13000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 14000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 15000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 16000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 17000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 18000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 19000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 20000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 21000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 22000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 23000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 24000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 25000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 26000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 27000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 28000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 29000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 30000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 31000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 32000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 33000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 34000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 35000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 36000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 37000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 38000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 39000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 40000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 41000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 42000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 43000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 44000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 45000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 46000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 47000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 48000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 49000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 50000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 51000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 52000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 53000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 54000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 55000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 56000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 57000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 58000 / 60000: avg loss = 0.000003\n",
      "Epoch 5, Itr 59000 / 60000: avg loss = 0.000003\n",
      "Epoch 5 / Test accuracy: 95.97%\n",
      "Epoch 6, Itr 0 / 60000: avg loss = 0.000168\n",
      "Epoch 6, Itr 1000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 2000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 3000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 4000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 5000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 6000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 7000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 8000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 9000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 10000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 11000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 12000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 13000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 14000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 15000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 16000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 17000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 18000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 19000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 20000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 21000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 22000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 23000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 24000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 25000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 26000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 27000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 28000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 29000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 30000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 31000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 32000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 33000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 34000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 35000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 36000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 37000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 38000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 39000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 40000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 41000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 42000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 43000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 44000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 45000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 46000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 47000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 48000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 49000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 50000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 51000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 52000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 53000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 54000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 55000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 56000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 57000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 58000 / 60000: avg loss = 0.000002\n",
      "Epoch 6, Itr 59000 / 60000: avg loss = 0.000002\n",
      "Epoch 6 / Test accuracy: 96.02%\n",
      "Epoch 7, Itr 0 / 60000: avg loss = 0.000061\n",
      "Epoch 7, Itr 1000 / 60000: avg loss = 0.000001\n",
      "Epoch 7, Itr 2000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 3000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 4000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 5000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 6000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 7000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 8000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 9000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 10000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 11000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 12000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 13000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 14000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 15000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 16000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 17000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 18000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 19000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 20000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 21000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 22000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 23000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 24000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 25000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 26000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 27000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 28000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 29000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 30000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 31000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 32000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 33000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 34000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 35000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 36000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 37000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 38000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 39000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 40000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 41000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 42000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 43000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 44000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 45000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 46000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 47000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 48000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 49000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 50000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 51000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 52000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 53000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 54000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 55000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 56000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 57000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 58000 / 60000: avg loss = 0.000002\n",
      "Epoch 7, Itr 59000 / 60000: avg loss = 0.000002\n",
      "Epoch 7 / Test accuracy: 96.81%\n",
      "Epoch 8, Itr 0 / 60000: avg loss = 0.000051\n",
      "Epoch 8, Itr 1000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 2000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 3000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 4000 / 60000: avg loss = 0.000001\n",
      "Epoch 8, Itr 5000 / 60000: avg loss = 0.000001\n",
      "Epoch 8, Itr 6000 / 60000: avg loss = 0.000001\n",
      "Epoch 8, Itr 7000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 8000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 9000 / 60000: avg loss = 0.000001\n",
      "Epoch 8, Itr 10000 / 60000: avg loss = 0.000001\n",
      "Epoch 8, Itr 11000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 12000 / 60000: avg loss = 0.000001\n",
      "Epoch 8, Itr 13000 / 60000: avg loss = 0.000001\n",
      "Epoch 8, Itr 14000 / 60000: avg loss = 0.000001\n",
      "Epoch 8, Itr 15000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 16000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 17000 / 60000: avg loss = 0.000001\n",
      "Epoch 8, Itr 18000 / 60000: avg loss = 0.000001\n",
      "Epoch 8, Itr 19000 / 60000: avg loss = 0.000001\n",
      "Epoch 8, Itr 20000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 21000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 22000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 23000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 24000 / 60000: avg loss = 0.000001\n",
      "Epoch 8, Itr 25000 / 60000: avg loss = 0.000001\n",
      "Epoch 8, Itr 26000 / 60000: avg loss = 0.000001\n",
      "Epoch 8, Itr 27000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 28000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 29000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 30000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 31000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 32000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 33000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 34000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 35000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 36000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 37000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 38000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 39000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 40000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 41000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 42000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 43000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 44000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 45000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 46000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 47000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 48000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 49000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 50000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 51000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 52000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 53000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 54000 / 60000: avg loss = 0.000001\n",
      "Epoch 8, Itr 55000 / 60000: avg loss = 0.000001\n",
      "Epoch 8, Itr 56000 / 60000: avg loss = 0.000001\n",
      "Epoch 8, Itr 57000 / 60000: avg loss = 0.000001\n",
      "Epoch 8, Itr 58000 / 60000: avg loss = 0.000002\n",
      "Epoch 8, Itr 59000 / 60000: avg loss = 0.000001\n",
      "Epoch 8 / Test accuracy: 97.07%\n",
      "Epoch 9, Itr 0 / 60000: avg loss = 0.000039\n",
      "Epoch 9, Itr 1000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 2000 / 60000: avg loss = 0.000002\n",
      "Epoch 9, Itr 3000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 4000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 5000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 6000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 7000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 8000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 9000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 10000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 11000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 12000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 13000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 14000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 15000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 16000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 17000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 18000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 19000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 20000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 21000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 22000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 23000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 24000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 25000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 26000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 27000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 28000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 29000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 30000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 31000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 32000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 33000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 34000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 35000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 36000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 37000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 38000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 39000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 40000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 41000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 42000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 43000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 44000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 45000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 46000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 47000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 48000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 49000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 50000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 51000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 52000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 53000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 54000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 55000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 56000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 57000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 58000 / 60000: avg loss = 0.000001\n",
      "Epoch 9, Itr 59000 / 60000: avg loss = 0.000001\n",
      "Epoch 9 / Test accuracy: 96.98%\n",
      "Epoch 10, Itr 0 / 60000: avg loss = 0.000099\n",
      "Epoch 10, Itr 1000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 2000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 3000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 4000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 5000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 6000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 7000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 8000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 9000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 10000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 11000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 12000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 13000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 14000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 15000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 16000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 17000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 18000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 19000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 20000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 21000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 22000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 23000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 24000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 25000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 26000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 27000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 28000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 29000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 30000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 31000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 32000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 33000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 34000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 35000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 36000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 37000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 38000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 39000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 40000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 41000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 42000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 43000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 44000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 45000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 46000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 47000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 48000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 49000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 50000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 51000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 52000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 53000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 54000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 55000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 56000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 57000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 58000 / 60000: avg loss = 0.000001\n",
      "Epoch 10, Itr 59000 / 60000: avg loss = 0.000001\n",
      "Epoch 10 / Test accuracy: 97.31%\n"
     ]
    }
   ],
   "source": [
    "# Initialize network parameters\n",
    "class MLP3:\n",
    "    def __init__(self, in_dim=784, h1=128, h2=64, out_dim=10, seed=42):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.W1 = Tensor(rng.normal(0, 0.01, (in_dim, h1)), requires_grad=True)\n",
    "        self.b1 = Tensor(np.zeros(h1, dtype=np.float32), requires_grad=True)\n",
    "        self.W2 = Tensor(rng.normal(0, 0.01, (h1, h2)), requires_grad=True)\n",
    "        self.b2 = Tensor(np.zeros(h2, dtype=np.float32), requires_grad=True)\n",
    "        self.W3 = Tensor(rng.normal(0, 0.01, (h2, out_dim)), requires_grad=True)\n",
    "        self.b3 = Tensor(np.zeros(out_dim, dtype=np.float32), requires_grad=True)\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return [self.W1, self.b1, self.W2, self.b2, self.W3, self.b3]\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        h1 = ReLU.apply(x @ self.W1 + self.b1)\n",
    "        h2 = ReLU.apply(h1 @ self.W2 + self.b2)\n",
    "        logits = h2 @ self.W3 + self.b3\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = MLP3()\n",
    "learning_rate = 0.1\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle training data\n",
    "    perm = np.random.permutation(X_train.shape[0])\n",
    "    X_train, y_train = X_train[perm], y_train[perm]\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        # Mini-batch slice\n",
    "        X_batch = Tensor(X_train[i : i + batch_size])  # input batch\n",
    "        y_batch = Tensor(y_train[i : i + batch_size])  # target batch\n",
    "\n",
    "        # Forward pass:\n",
    "        logits = model(X_batch)\n",
    "        probs = Softmax.apply(logits)\n",
    "        logp = Log.apply(probs)\n",
    "        loss = NLLLoss.apply(logp, y_batch)\n",
    "        # loss = CrossEntropyLoss.apply(logits, y_batch)  # Compute cross-entropy loss over the batch\n",
    "        total_loss += float(loss.data)\n",
    "\n",
    "        # Backward pass:\n",
    "        loss.backward()  # compute gradients for all weights/biases\n",
    "\n",
    "        # Update parameters with SGD:\n",
    "        for param in model.params:\n",
    "            # simple gradient descent step\n",
    "            param.data -= learning_rate * param.grad\n",
    "            # reset gradient to zero for next batch\n",
    "            param.grad = None\n",
    "\n",
    "        # Evaluate accuracy on the training set (optional) or just print loss\n",
    "        if ((i) % 1000) == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}, Itr {i} / {X_train.shape[0]}: avg loss = {total_loss / (X_train.shape[0] / batch_size * (i + 1)):.6f}\"\n",
    "            )\n",
    "\n",
    "    # quick test accuracy\n",
    "    Xt = Tensor(X_test, requires_grad=False)\n",
    "    logits = model(Xt)\n",
    "    preds = np.argmax(logits.data, axis=1)\n",
    "    true = np.argmax(y_test, axis=1)\n",
    "    acc = (preds == true).mean()\n",
    "    print(f\"Epoch {epoch + 1} / Test accuracy: {acc * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
