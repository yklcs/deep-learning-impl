{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5819a4f-627e-46c7-b41e-e5544e9f32b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Context:\n",
    "    \"\"\"Context to save variables for backward computation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.saved_tensors = ()\n",
    "\n",
    "    def save_for_backward(self, *tensors):\n",
    "        self.saved_tensors = tuple(tensors)\n",
    "\n",
    "\n",
    "class Op:\n",
    "    @staticmethod\n",
    "    def forward(ctx, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @classmethod\n",
    "    def apply(cls, *inputs):\n",
    "        ctx = Context()\n",
    "        raw_inputs = [x.data if isinstance(x, Tensor) else x for x in inputs]\n",
    "        result_data = cls.forward(ctx, *raw_inputs)\n",
    "\n",
    "        requires_grad = any(isinstance(x, Tensor) and x.requires_grad for x in inputs)\n",
    "        out = Tensor(result_data, requires_grad=requires_grad)\n",
    "        if requires_grad:\n",
    "            out.grad_fn = (cls, ctx, inputs)\n",
    "        return out\n",
    "\n",
    "\n",
    "def reduce_grad_to_shape(grad, shape):\n",
    "    \"\"\"\n",
    "    Match the gradient dimension for the input tensor\n",
    "    - grad: output gradient\n",
    "    - shape: original input tensor shape\n",
    "    \"\"\"\n",
    "    # remove expanded dimension for batched data\n",
    "    while grad.ndim > len(shape):\n",
    "        grad = grad.sum(axis=0)\n",
    "\n",
    "    # remove the broadcasted axis\n",
    "    for i, dim in enumerate(shape):\n",
    "        if dim == 1 and (grad.shape[i] != 1):\n",
    "            grad = grad.sum(axis=i, keepdims=True)\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "class Add(Op):\n",
    "    @staticmethod\n",
    "    def forward(ctx, a, b):\n",
    "        ctx.save_for_backward(a.shape, b.shape)\n",
    "        return a + b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a_shape, b_shape = ctx.saved_tensors\n",
    "        grad_a = reduce_grad_to_shape(grad_output, a_shape)\n",
    "        grad_b = reduce_grad_to_shape(grad_output, b_shape)\n",
    "        return grad_a, grad_b\n",
    "\n",
    "\n",
    "class Mul(Op):\n",
    "    @staticmethod\n",
    "    def forward(ctx, a, b):\n",
    "        ctx.save_for_backward(a, b)\n",
    "        return a * b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_a = b * reduce_grad_to_shape(grad_output, a.shape)\n",
    "        grad_b = a * reduce_grad_to_shape(grad_output, b.shape)\n",
    "        return grad_a, grad_b\n",
    "\n",
    "\n",
    "class MatMul(Op):\n",
    "    @staticmethod\n",
    "    def forward(ctx, a, b):\n",
    "        ctx.save_for_backward(a, b)\n",
    "        return a @ b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_a = grad_output @ b.T\n",
    "        grad_b = a.T @ grad_output\n",
    "        return grad_a, grad_b\n",
    "\n",
    "\n",
    "class Pow(Op):\n",
    "    @staticmethod\n",
    "    def forward(ctx, a, b):\n",
    "        ctx.save_for_backward(a, b)\n",
    "        return a**b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_a = grad_output * a ** (b - 1) * b\n",
    "        grad_b = grad_output * a**b * np.log(a)\n",
    "        return grad_a, grad_b\n",
    "\n",
    "\n",
    "class Sum(Op):\n",
    "    @staticmethod\n",
    "    def forward(ctx, a, axis=None, keepdims=False):\n",
    "        ctx.save_for_backward(a.shape, axis, keepdims)\n",
    "        return a.sum(axis=None, keepdims=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a_shape, axis, keepdims = ctx.saved_tensors\n",
    "        grad_a = reduce_grad_to_shape(\n",
    "            grad_output.sum(axis=axis, keepdims=keepdims), a_shape\n",
    "        )\n",
    "        return grad_a\n",
    "\n",
    "\n",
    "class ReLU(Op):\n",
    "    @staticmethod\n",
    "    def forward(ctx, a):\n",
    "        ctx.save_for_backward(a)\n",
    "        return np.max(a, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a = ctx.saved_tensors\n",
    "        grad_a = reduce_grad_to_shape(grad_output, a.shape).copy()\n",
    "        grad_a[a > 0] *= 1\n",
    "        grad_a[a <= 0] *= 0\n",
    "\n",
    "\n",
    "class CrossEntropyLoss(Op):\n",
    "    @staticmethod\n",
    "    def forward(ctx, logits, targets):\n",
    "        ctx.save_for_backward(logits, targets)\n",
    "        return np.sum(-targets * logits.log())\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        logits, targets = ctx.saved_tensors\n",
    "\n",
    "\n",
    "class Log(Op):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class NLLLoss(Op):\n",
    "    @staticmethod\n",
    "    def forward(ctx, log_probs, targets_one_hot):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Softmax(Op):\n",
    "    @staticmethod\n",
    "    def forward(ctx, logits):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "625bfb31-eebe-4060-9a05-fa2df6bfa2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, requires_grad=False):\n",
    "        if isinstance(data, Tensor):\n",
    "            data = data.data  # unwrap\n",
    "        self.data = np.array(data, dtype=np.float32)\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = None\n",
    "        self.grad_fn = None  # (OpClass, ctx, inputs) for non-leaf tensors\n",
    "\n",
    "    # Operator overloads delegate to our Op classes via apply:\n",
    "    def __add__(self, other):\n",
    "        return Add.apply(self, other)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return Add.apply(other, self)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return Add.apply(self, Mul.apply(other, Tensor(-1.0)))\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return Add.apply(other, Mul.apply(self, Tensor(-1.0)))\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Mul.apply(self, other)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return Mul.apply(other, self)\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        return MatMul.apply(self, other)  # matrix @\n",
    "\n",
    "    def __neg__(self):\n",
    "        return Mul.apply(self, Tensor(-1.0))\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return Mul.apply(self, Tensor(1.0) / other)\n",
    "\n",
    "    def __pow__(self, exponent):\n",
    "        return Pow.apply(self, exponent)\n",
    "\n",
    "    def __rpow__(self, base):\n",
    "        return Pow.apply(base, self)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(data={self.data}, grad={self.grad}, requires_grad={self.requires_grad})\"\n",
    "\n",
    "    # initialization\n",
    "    @staticmethod\n",
    "    def zeros(shape, requires_grad=False):\n",
    "        return Tensor(np.zeros(shape, dtype=np.float32), requires_grad=requires_grad)\n",
    "\n",
    "    @staticmethod\n",
    "    def ones(shape, requires_grad=False):\n",
    "        return Tensor(np.ones(shape, dtype=np.float32), requires_grad=requires_grad)\n",
    "\n",
    "    def backward(self, grad_output=None):\n",
    "        if not self.requires_grad:\n",
    "            raise RuntimeError(\n",
    "                \"Cannot call backward on a tensor that does not require grad.\"\n",
    "            )\n",
    "        # If no grad specified, tensor must be scalar\n",
    "        if grad_output is None:\n",
    "            if self.data.size != 1:\n",
    "                raise RuntimeError(\n",
    "                    \"grad_output must be provided for non-scalar tensors\"\n",
    "                )\n",
    "            grad_output = np.ones_like(self.data, dtype=np.float32)\n",
    "        else:\n",
    "            grad_output = np.array(grad_output, dtype=np.float32)\n",
    "        # Initialize this tensor's gradient\n",
    "        self.grad = grad_output\n",
    "\n",
    "        # Build a topologically sorted list of tensors (post-order DFS)\n",
    "        topo_order = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_graph(t):\n",
    "            if isinstance(t, Tensor) and t not in visited:\n",
    "                visited.add(t)\n",
    "                if t.grad_fn is not None:  # not a leaf\n",
    "                    op_cls, ctx, inputs = t.grad_fn\n",
    "                    for inp in inputs:\n",
    "                        build_graph(inp)\n",
    "                topo_order.append(t)\n",
    "\n",
    "        build_graph(self)\n",
    "\n",
    "        # Traverse graph in reverse topological order, apply chain rule\n",
    "        for t in reversed(topo_order):\n",
    "            if t.grad_fn is None:\n",
    "                continue  # leaf node (no backward op)\n",
    "            op_cls, ctx, inputs = t.grad_fn\n",
    "            grad_out = t.grad  # gradient of the output w.rt. this tensor\n",
    "            # Compute gradients of inputs via this op's backward\n",
    "            grad_inputs = op_cls.backward(ctx, grad_out)\n",
    "            if grad_inputs is None:\n",
    "                grad_inputs = ()\n",
    "            elif not isinstance(grad_inputs, tuple):\n",
    "                grad_inputs = (grad_inputs,)\n",
    "            # Accumulate gradients into input tensors\n",
    "            for inp, grad in zip(inputs, grad_inputs):\n",
    "                if isinstance(inp, Tensor) and inp.requires_grad and grad is not None:\n",
    "                    grad = np.array(grad, dtype=np.float32)  # ensure numpy\n",
    "                    if inp.grad is None:\n",
    "                        inp.grad = grad\n",
    "                    else:\n",
    "                        inp.grad += grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05c4af3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "OurTensor = Tensor\n",
    "TorchTensor = torch.Tensor\n",
    "\n",
    "x = np.random.rand(3, 4)\n",
    "y = np.random.rand(4, 5)\n",
    "\n",
    "\n",
    "# Torch\n",
    "z = TorchTensor(x) @ TorchTensor(y)\n",
    "z.requires_grad = True\n",
    "z.sum().backward()\n",
    "print(z.grad)\n",
    "\n",
    "# Ours\n",
    "z = OurTensor(x) @ OurTensor(y)\n",
    "z.requires_grad = True\n",
    "Sum.apply(z).backward()\n",
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b23dd492-74d7-4765-af1c-b64f031e0775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Autograd Graph (parents -> node) ===\n",
      "[f6210] Leaf(shape=(), requires_grad=True)\n",
      "[4e510] Leaf(shape=(), requires_grad=True)\n",
      "[f6210] Leaf(shape=(), requires_grad=True) --Mul--> [71c40] Mul(out_shape=())\n",
      "[4e510] Leaf(shape=(), requires_grad=True) --Mul--> [71c40] Mul(out_shape=())\n",
      "[2c350] Leaf(shape=(), requires_grad=True)\n",
      "[71c40] Mul(out_shape=()) --Add--> [702f0] Add(out_shape=())\n",
      "[2c350] Leaf(shape=(), requires_grad=True) --Add--> [702f0] Add(out_shape=())\n",
      "[702f0] Add(out_shape=()) --ReLU--> [735c0] ReLU(out_shape=())\n",
      "\n",
      "=== Backward visit order (reverse topological) ===\n",
      "[735c0] ReLU(out_shape=())\n",
      "[702f0] Add(out_shape=())\n",
      "[2c350] Leaf(shape=(), requires_grad=True)\n",
      "[71c40] Mul(out_shape=())\n",
      "[4e510] Leaf(shape=(), requires_grad=True)\n",
      "[f6210] Leaf(shape=(), requires_grad=True)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     66\u001b[39m loss = y  \u001b[38;5;66;03m# 스칼라처럼 취급 (여기선 y가 스칼라)\u001b[39;00m\n\u001b[32m     68\u001b[39m print_autograd_graph(loss)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Gradients ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     72\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdl/dx =\u001b[39m\u001b[33m\"\u001b[39m, x.grad)  \u001b[38;5;66;03m# ReLU'(z) * w  (z>0이면 w, 아니면 0)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, grad_output)\u001b[39m\n\u001b[32m     93\u001b[39m grad_out = t.grad  \u001b[38;5;66;03m# gradient of the output w.rt. this tensor\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Compute gradients of inputs via this op's backward\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m grad_inputs = \u001b[43mop_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grad_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     97\u001b[39m     grad_inputs = ()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 134\u001b[39m, in \u001b[36mReLU.backward\u001b[39m\u001b[34m(ctx, grad_output)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward\u001b[39m(ctx, grad_output):\n\u001b[32m    133\u001b[39m     a = ctx.saved_tensors\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     grad_a = reduce_grad_to_shape(grad_output, \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m).copy()\n\u001b[32m    135\u001b[39m     grad_a[a > \u001b[32m0\u001b[39m] *= \u001b[32m1\u001b[39m\n\u001b[32m    136\u001b[39m     grad_a[a <= \u001b[32m0\u001b[39m] *= \u001b[32m0\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "def _node_name(t: \"Tensor\"):\n",
    "    if t.grad_fn is None:\n",
    "        return f\"Leaf(shape={tuple(t.data.shape)}, requires_grad={t.requires_grad})\"\n",
    "    op_cls, _, _ = t.grad_fn\n",
    "    return f\"{op_cls.__name__}(out_shape={tuple(t.data.shape)})\"\n",
    "\n",
    "\n",
    "def _short_id(obj):\n",
    "    return hex(id(obj))[-5:]  # 보기 좋은 짧은 id\n",
    "\n",
    "\n",
    "def trace_graph(root: \"Tensor\"):\n",
    "    \"\"\"루트 텐서에서 시작해 모든 조상(부모 텐서)들을 수집하고\n",
    "    토폴로지 순서를 반환합니다.\"\"\"\n",
    "    visited = set()\n",
    "    topo = []\n",
    "\n",
    "    def build(t):\n",
    "        if not isinstance(t, Tensor) or t in visited:\n",
    "            return\n",
    "        visited.add(t)\n",
    "        if t.grad_fn is not None:\n",
    "            op_cls, ctx, inputs = t.grad_fn\n",
    "            for inp in inputs:\n",
    "                if isinstance(inp, Tensor):\n",
    "                    build(inp)\n",
    "        topo.append(t)\n",
    "\n",
    "    build(root)\n",
    "    return topo  # 부모가 먼저, root가 마지막(포스트오더)\n",
    "\n",
    "\n",
    "def print_autograd_graph(root: \"Tensor\"):\n",
    "    topo = trace_graph(root)\n",
    "    print(\"=== Autograd Graph (parents -> node) ===\")\n",
    "    for node in topo:\n",
    "        label_node = _node_name(node)\n",
    "        nid = _short_id(node)\n",
    "        if node.grad_fn is None:\n",
    "            print(f\"[{nid}] {label_node}\")\n",
    "            continue\n",
    "        op_cls, _, inputs = node.grad_fn\n",
    "        # 부모들 나열 (부모) --op--> (자식=node)\n",
    "        for inp in inputs:\n",
    "            if isinstance(inp, Tensor):\n",
    "                pid = _short_id(inp)\n",
    "                print(\n",
    "                    f\"[{pid}] {_node_name(inp)} --{op_cls.__name__}--> [{nid}] {label_node}\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"[const {inp}] --{op_cls.__name__}--> [{nid}] {label_node}\")\n",
    "\n",
    "    # backward 방문 순서(실제로 backward에서 사용하는 역토폴로지)\n",
    "    print(\"\\n=== Backward visit order (reverse topological) ===\")\n",
    "    for node in reversed(topo):\n",
    "        print(f\"[{_short_id(node)}] {_node_name(node)}\")\n",
    "\n",
    "\n",
    "# f(x, w, b) = ReLU(x*w + b)\n",
    "x = Tensor(2.0, requires_grad=True)\n",
    "w = Tensor(3.0, requires_grad=True)\n",
    "b = Tensor(-4.0, requires_grad=True)\n",
    "\n",
    "z = x * w + b  # Add(Mul(x,w), b)\n",
    "y = ReLU.apply(z)  # ReLU(z)\n",
    "loss = y  # 스칼라처럼 취급 (여기선 y가 스칼라)\n",
    "\n",
    "print_autograd_graph(loss)\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\n=== Gradients ===\")\n",
    "print(\"dl/dx =\", x.grad)  # ReLU'(z) * w  (z>0이면 w, 아니면 0)\n",
    "print(\"dl/dw =\", w.grad)  # ReLU'(z) * x\n",
    "print(\"dl/db =\", b.grad)  # ReLU'(z) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d469f0-5a90-4ffa-99c7-127955807281",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Tensor(2.0, requires_grad=True)\n",
    "w = Tensor(3.0, requires_grad=True)\n",
    "b = Tensor(-4.0, requires_grad=True)\n",
    "\n",
    "z = x * w + b  # Add(Mul(x,w), b)\n",
    "y = ReLU.apply(z)  # ReLU(z)\n",
    "loss = y  # 스칼라처럼 취급 (여기선 y가 스칼라)\n",
    "\n",
    "print_autograd_graph(loss)\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\n=== Gradients ===\")\n",
    "print(\"dl/dx =\", x.grad)  # ReLU'(z) * w  (z>0이면 w, 아니면 0)\n",
    "print(\"dl/dw =\", w.grad)  # ReLU'(z) * x\n",
    "print(\"dl/db =\", b.grad)  # ReLU'(z) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e4d2b-ca5f-4811-82ba-a37e00555854",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor(np.array([0.4, 1.0, -2.0], dtype=np.float32), requires_grad=True)\n",
    "\n",
    "y = ReLU.apply(a * Tensor(2.0) - Tensor(1.0))  # elementwise\n",
    "loss = Sum.apply(y)  # Sum op로 스칼라화\n",
    "\n",
    "print_autograd_graph(loss)\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\n=== Values & Gradients ===\")\n",
    "print(\"y =\", y.data)  # [0., 1., 0.]  (ReLU 결과)\n",
    "print(\"dl/da =\", a.grad)  # ReLU>0인 위치에서 2, 나머지는 0  → [0., 2., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eecd87-5df5-40dd-93f1-f909d28871bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gzip\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "\n",
    "def load_mnist(path=None):\n",
    "    \"\"\"Download MNIST and load it into NumPy arrays.\"\"\"\n",
    "    # url_base = \"http://yann.lecun.com/exdb/mnist/\"\n",
    "    url_base = \"https://ossci-datasets.s3.amazonaws.com/mnist/\"\n",
    "\n",
    "    files = {\n",
    "        \"train_images\": \"train-images-idx3-ubyte.gz\",\n",
    "        \"train_labels\": \"train-labels-idx1-ubyte.gz\",\n",
    "        \"test_images\": \"t10k-images-idx3-ubyte.gz\",\n",
    "        \"test_labels\": \"t10k-labels-idx1-ubyte.gz\",\n",
    "    }\n",
    "    # Default path to ./data/mnist\n",
    "    if path is None:\n",
    "        path = os.path.join(os.path.expanduser(\"./\"), \"data\", \"mnist\")\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Download missing files\n",
    "    for name, filename in files.items():\n",
    "        filepath = os.path.join(path, filename)\n",
    "        if not os.path.isfile(filepath):\n",
    "            urlretrieve(url_base + filename, filepath)\n",
    "            print(f\"Downloaded {filename}\")\n",
    "\n",
    "    # Load images\n",
    "    def _read_images(filename):\n",
    "        with gzip.open(os.path.join(path, filename), \"rb\") as f:\n",
    "            data = f.read()\n",
    "            # The first 16 bytes are header (magic, num, rows, cols)\n",
    "            images = np.frombuffer(data, dtype=np.uint8, offset=16)\n",
    "        images = images.reshape(-1, 28 * 28).astype(np.float32) / 255.0\n",
    "        return images\n",
    "\n",
    "    # Load labels\n",
    "    def _read_labels(filename):\n",
    "        with gzip.open(os.path.join(path, filename), \"rb\") as f:\n",
    "            data = f.read()\n",
    "            # First 8 bytes are header (magic, num)\n",
    "            labels = np.frombuffer(data, dtype=np.uint8, offset=8)\n",
    "        # Convert to one-hot vectors of length 10\n",
    "        one_hot = np.zeros((labels.size, 10), dtype=np.float32)\n",
    "        one_hot[np.arange(labels.size), labels] = 1.0\n",
    "        return one_hot\n",
    "\n",
    "    # Read all parts\n",
    "    X_train = _read_images(files[\"train_images\"])\n",
    "    y_train = _read_labels(files[\"train_labels\"])\n",
    "    X_test = _read_images(files[\"test_images\"])\n",
    "    y_test = _read_labels(files[\"test_labels\"])\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "# Load the MNIST data (this will download if not already present)\n",
    "X_train, y_train, X_test, y_test = load_mnist()\n",
    "print(\"MNIST data loaded:\", X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab29554-1619-432e-87e5-7919b4e1de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network parameters\n",
    "class MLP3:\n",
    "    def __init__(self, in_dim=784, h1=128, h2=64, out_dim=10, seed=42):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.W1 = Tensor(rng.normal(0, 0.01, (in_dim, h1)), requires_grad=True)\n",
    "        self.b1 = Tensor(np.zeros(h1, dtype=np.float32), requires_grad=True)\n",
    "        self.W2 = Tensor(rng.normal(0, 0.01, (h1, h2)), requires_grad=True)\n",
    "        self.b2 = Tensor(np.zeros(h2, dtype=np.float32), requires_grad=True)\n",
    "        self.W3 = Tensor(rng.normal(0, 0.01, (h2, out_dim)), requires_grad=True)\n",
    "        self.b3 = Tensor(np.zeros(out_dim, dtype=np.float32), requires_grad=True)\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return [self.W1, self.b1, self.W2, self.b2, self.W3, self.b3]\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        h1 = ReLU.apply(x @ self.W1 + self.b1)\n",
    "        h2 = ReLU.apply(h1 @ self.W2 + self.b2)\n",
    "        logits = h2 @ self.W3 + self.b3\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = MLP3()\n",
    "learning_rate = 0.1\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle training data\n",
    "    perm = np.random.permutation(X_train.shape[0])\n",
    "    X_train, y_train = X_train[perm], y_train[perm]\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        # Mini-batch slice\n",
    "        X_batch = Tensor(X_train[i : i + batch_size])  # input batch\n",
    "        y_batch = Tensor(y_train[i : i + batch_size])  # target batch\n",
    "\n",
    "        # Forward pass:\n",
    "        logits = model(X_batch)\n",
    "        probs = Softmax.apply(logits)\n",
    "        logp = Log.apply(probs)\n",
    "        loss = NLLLoss.apply(logp, y_batch)\n",
    "        # loss = CrossEntropyLoss.apply(logits, y_batch)  # Compute cross-entropy loss over the batch\n",
    "        total_loss += float(loss.data)\n",
    "\n",
    "        # Backward pass:\n",
    "        loss.backward()  # compute gradients for all weights/biases\n",
    "\n",
    "        # Update parameters with SGD:\n",
    "        for param in model.params:\n",
    "            # simple gradient descent step\n",
    "            param.data -= learning_rate * param.grad\n",
    "            # reset gradient to zero for next batch\n",
    "            param.grad = None\n",
    "\n",
    "        # Evaluate accuracy on the training set (optional) or just print loss\n",
    "        if ((i) % 1000) == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}, Itr {i} / {X_train.shape[0]}: avg loss = {total_loss / (X_train.shape[0] / batch_size * (i + 1)):.6f}\"\n",
    "            )\n",
    "\n",
    "    # quick test accuracy\n",
    "    Xt = Tensor(X_test, requires_grad=False)\n",
    "    logits = model(Xt)\n",
    "    preds = np.argmax(logits.data, axis=1)\n",
    "    true = np.argmax(y_test, axis=1)\n",
    "    acc = (preds == true).mean()\n",
    "    print(f\"Epoch {epoch + 1} / Test accuracy: {acc * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
